---
title: $K$NN
author: "Jon Garrow"
date: "02/10/2025"

format: 
  html: 
    theme: superhero  
    mainfont: monospace
    highlight-style: github
    title-block-banner: true
    embed-resources: true

---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](https://raw.githubusercontent.com/cd-public/D505/refs/heads/master/hws/src/knn.qmd) hosted on GitHub pages.

# 0. Quarto Type-setting

- This document is rendered with Quarto, and configured to embed an images using the `embed-resources` option in the header.
- If you wish to use a similar header, here's is the format specification for this document:

```email
format: 
  html:
    embed-resources: true
```

# 1. Setup

```{r}
library(tidyverse)
library(caret)
library(EnvStats)
library(fastDummies)
wine <- readRDS(gzcon(url("https://github.com/cd-public/D505/raw/master/dat/pinot.rds")))
```

## 2. $K$NN Concepts

> <span style="color:red;font-weight:bold">TODO</span>: An even number for K should not be used as it could result in a tie for nearest neighbor. The K should be chosen based on the sample set, as the result will be an average of the chosen neighbors. Large datasets should not have a very small K, and small datasets would not perform well with an overlarge K.

## 3. Feature Engineering

1. Create a version of the year column that is a *factor* (instead of numeric).
2. Create dummy variables that indicate the presence of "cherry", "chocolate" and "earth" in the description.
  - Take care to handle upper and lower case characters.
3. Create 3 new features that represent the interaction between *time* and the cherry, chocolate and earth inidicators.
4. Remove the description column from the data.

```{r}
wine <- wine %>% mutate(yearf=as.factor(year)) #Create new variable with year as a factor

wine <- wine %>% mutate(
  cherry=str_detect(description, "[Cc]herry"), # cherry logic var
  choco=str_detect(description, "[Cc]hocolate"), # create choco logic var
  earth=str_detect(description, "[Ee]arth"), # create earth logic var
  yearf_cherry = interaction(yearf, cherry), # interaction between yearf and cherry
  yearf_choco = interaction(yearf, choco), # interaction between yearf and choco
  yearf_earth = interaction(yearf, earth) # interaction between yearf and earf
  )
wine <- wine %>% select(-description) # zap description var now that we've coded cherry, choco, and eARTh

summary(wine)
```
## 4. Preprocessing

1. Preprocess the dataframe from the previous code block using BoxCox, centering and scaling of the numeric features
2. Create dummy variables for the `year` factor column

```{r}

# box_wine <- boxcoxTransform(wine)
# print(box_wine)

wine %>% 
  preProcess(method = c("BoxCox","center","scale")) %>% # CONFIRM this is the method to use here
  predict(wine) %>% 
  head()

wine <- dummy_cols(wine, select_columns = "yearf", remove_first_dummy = TRUE)
head(wine) # create dummy vars 
```


## 5. Running $K$NN

1. Split the dataframe into an 80/20 training and test set
2. Use Caret to run a $K$NN model that uses our engineered features to predict province
  - use 5-fold cross validated subsampling 
  - allow Caret to try 15 different values for $K$
3. Display the confusion matrix on the test data


```{r}
windex <- createDataPartition(wine$province, p = 0.8, list = FALSE) # partition wine df into 80/20 using province var

set.seed(505) # set seed in case we need to recreate partioning 
wine_train <- wine[windex, ] # assign partitioned 80% to training df
wine_test <- wine[-windex, ] # assign remaining 20% to testing df

 model <- train(
  province ~ .,
  data = wine_train,
  method = "knn",
  metric = "Kappa",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 15)

predicted <- predict(model, wine_test) # apply model to test data
confusionMatrix(predicted,factor(wine_test$province))$overall # view confusion matrix for knn model predictions
```

## 6. Kappa

How do we determine whether a Kappa value represents a good, bad or some other outcome?

> <span style="color:red;font-weight:bold">TODO</span>: Cohen interprets Kappa values in bins, with .6-.8 being good, .4-.6 moderate, and below .4 fair. Higher than .8 is likely overfitted, and realistically below .5 is not a good value. Kappa indicates the likelihood of matching neighbors at random, vs. as modeled. 

## 7. Improvement

How can we interpret the confusion matrix, and how can we improve in our predictions?

> <span style="color:red;font-weight:bold">TODO</span>: The confusion matrix has a Kappa value of .18, a very low value for this model. This model is very ineffective at correctly identifying the province based on the features included. This appears to be over-identifying California, which is a large proportion of the training dataset. The predictions might be improved with a more balanced dataset, and likely the features could be changed to improve predictions.